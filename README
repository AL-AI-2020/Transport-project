# SViTT: Temporal Learning of Sparse Video-Text Transformers (CVPR 2023)

[Yi Li](http://www.svcl.ucsd.edu/people/yili)<sup>1</sup>, [Kyle Min](https://sites.google.com/view/kylemin)<sup>2</sup>, [Subarna Tripathi](https://subarnatripathi.github.io/)<sup>2</sup>, [Nuno Vasconcelos](http://www.svcl.ucsd.edu/~nuno)<sup>1</sup>

<sup>1</sup>University of California, San Diego, <sup>2</sup>Intel Labs

**[Project page](http://svcl.ucsd.edu/projects/svitt/) | [Paper](https://arxiv.org/abs/2304.08809) | [8-min video](https://www.youtube.com/watch?v=11MZj4xkZyY)**

This repository contains PyTorch implementation of **`SViTT`**, a sparse multimodal transformer for video-language learning.

<p align="center">
  <img src="assets/svitt-arch.png" width="100%">
</p>

## Get started

```bash
conda env create -n svitt --file environment.yml
conda activate svitt
```

### Data

All datasets are expected under `data/` directory with the following structure (other downstream datasets follow the same structure as MSRVTT):
```
data/
├── anno_pretrain/
│   └── webvid_train.json
├── anno_downstream/
│   ├── msrvtt_test1k.json
│   └── ...
├── webvid_videos/
│   └── *.mp4
├── msrvtt_videos/
│   └── *.mp4
└── ...
```

Raw videos should be downloaded from the websites of respective datasets. Annotations for pre-training and downstream tasks are available in the [Singularity](https://github.com/jayleicn/singularity/tree/main#annotations) repo; additional annotations for Charades and AGQA used in this work are available [here](https://nextcloud.nrp-nautilus.io/s/DBkBXnQsZaeoSzQ).

